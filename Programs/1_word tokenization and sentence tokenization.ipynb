{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d37815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#two types are word tokenization and sentence tokezination, main use is keyword identification,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39797612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy is a object oriented library and nltk is a string oriented library, matching version of the spacy library\n",
    "# en_core_web_sm  is most commonly used , \n",
    "# the text will be given to the model, core has a by default tokenizer,it will convert it to a document objects \n",
    "#  called doc objects,the model also has several pipelines called togger, ner ,parser,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b373e49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word token:  ['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'data.', 'We', 'will', 'be', 'considering', 'pythons', 'inbuilt', 'method.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language,', 'library', 'and', 'purpose', 'of', 'modelling.', 'There', 'are', '5', 'different', 'types', 'in', 'it.', 'Characters', 'like', 'periods,', 'exclamation,', 'point', 'and', 'newline', 'are', 'used', 'to', 'separate', 'the', 'sentences.']\n",
      "\n",
      "\n",
      "Sentence token:  ['There are multiple ways we can perform tokenization on given data', ' We will be considering pythons inbuilt method', ' We can choose any method based on language, library and purpose of modelling', ' There are 5 different types in it', ' Characters like periods, exclamation, point and newline are used to separate the sentences', '']\n"
     ]
    }
   ],
   "source": [
    "text=\"\"\"There are multiple ways we can perform tokenization on given data. We will be considering pythons inbuilt method. We can choose any method based on language, library and purpose of modelling. There are 5 different types in it. Characters like periods, exclamation, point and newline are used to separate the sentences.\"\"\"\n",
    "word=text.split()  #split based on the spaces\n",
    "sent=text.split(\".\")\n",
    "print('Word token: ', word)\n",
    "print('\\n')\n",
    "print('Sentence token: ', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81c9f64",
   "metadata": {},
   "source": [
    "# Tokenization using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03cd387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word token:  ['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'data', 'We', 'will', 'be', 'considering', 'pythons', 'inbuilt', 'method', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language', 'library', 'and', 'purpose', 'of', 'modelling', 'There', 'are', '5', 'different', 'types', 'in', 'it', 'Characters', 'like', 'periods', 'exclamation', 'point', 'and', 'newline', 'are', 'used', 'to', 'separate', 'the', 'sentences']\n",
      "\n",
      "\n",
      "Sentence token:  ['There are multiple ways we can perform tokenization on given data', ' We will be considering pythons inbuilt method', ' We can choose any method based on language, library and purpose of modelling', ' There are 5 different types in it', ' Characters like periods, exclamation, point and newline are used to separate the sentences', '']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word=re.findall('[\\w]+', text)\n",
    "sent=re.compile('[.]').split(text)\n",
    "print('Word token: ', word)\n",
    "print('\\n')\n",
    "print('Sentence token: ', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165057c9",
   "metadata": {},
   "source": [
    "# Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ca8f48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/student/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/student/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/student/.local/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /home/student/.local/lib/python3.10/site-packages (from nltk) (4.66.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e647459a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Regex in /home/student/.local/lib/python3.10/site-packages (2023.12.25)\n"
     ]
    }
   ],
   "source": [
    "!pip install Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63902f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt to /home/student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af9cf57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word token:  ['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'data', '.', 'We', 'will', 'be', 'considering', 'pythons', 'inbuilt', 'method', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language', ',', 'library', 'and', 'purpose', 'of', 'modelling', '.', 'There', 'are', '5', 'different', 'types', 'in', 'it', '.', 'Characters', 'like', 'periods', ',', 'exclamation', ',', 'point', 'and', 'newline', 'are', 'used', 'to', 'separate', 'the', 'sentences', '.']\n",
      "\n",
      "\n",
      "Sentence token:  ['There are multiple ways we can perform tokenization on given data.', 'We will be considering pythons inbuilt method.', 'We can choose any method based on language, library and purpose of modelling.', 'There are 5 different types in it.', 'Characters like periods, exclamation, point and newline are used to separate the sentences.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "word=word_tokenize(text)\n",
    "sent=sent_tokenize(text)\n",
    "print('Word token: ', word)\n",
    "print('\\n')\n",
    "print('Sentence token: ', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced27fe",
   "metadata": {},
   "source": [
    "# Tokenization using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee66620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/student/.local/lib/python3.10/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/student/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/student/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/student/.local/lib/python3.10/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/student/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/student/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/student/.local/lib/python3.10/site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/student/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/student/.local/lib/python3.10/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/student/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/student/.local/lib/python3.10/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/student/.local/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/student/.local/lib/python3.10/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/student/.local/lib/python3.10/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/student/.local/lib/python3.10/site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/student/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/student/.local/lib/python3.10/site-packages (from spacy) (1.26.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/student/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/student/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/student/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/student/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/student/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/student/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7c2f7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m735.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /home/student/.local/lib/python3.10/site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.25.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/student/.local/lib/python3.10/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/student/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /home/student/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/student/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/student/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/student/.local/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.3)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /home/student/.local/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77c965d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word token:  ['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'data', '.', 'We', 'will', 'be', 'considering', 'pythons', 'inbuilt', 'method', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'language', ',', 'library', 'and', 'purpose', 'of', 'modelling', '.', 'There', 'are', '5', 'different', 'types', 'in', 'it', '.', 'Characters', 'like', 'periods', ',', 'exclamation', ',', 'point', 'and', 'newline', 'are', 'used', 'to', 'separate', 'the', 'sentences', '.']\n",
      "\n",
      "\n",
      "Sentence token:  ['There are multiple ways we can perform tokenization on given data.', 'We will be considering pythons inbuilt method.', 'We can choose any method based on language, library and purpose of modelling.', 'There are 5 different types in it.', 'Characters like periods, exclamation, point and newline are used to separate the sentences.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp(text)\n",
    "\n",
    "word=[]\n",
    "for t in doc:\n",
    "    word.append(t.text)\n",
    "    \n",
    "sent=[]\n",
    "for t in doc.sents:\n",
    "    sent.append(t.text)\n",
    "    \n",
    "print('Word token: ', word)\n",
    "print('\\n')\n",
    "print('Sentence token: ', sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27a86fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "There"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#basics of spacy\n",
    "doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4353762f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token=doc[1]\n",
    "token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04522da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17a5af30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33398bb",
   "metadata": {},
   "source": [
    "# Tokenization using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eab83f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /home/student/.local/lib/python3.10/site-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df0f497d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/student/.local/lib/python3.10/site-packages (2.15.0.post1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (1.26.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.13.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (0.35.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/student/.local/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/student/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/student/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/student/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/student/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/student/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/student/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/student/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/student/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/student/.local/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/student/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.4)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/student/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4cd435c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-09 10:00:43.380105: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-09 10:00:43.403803: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-09 10:00:43.540885: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-09 10:00:43.540991: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-09 10:00:43.568050: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-09 10:00:43.627877: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-09 10:00:43.629138: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-09 10:00:44.229093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word token:  ['there', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'data', 'we', 'will', 'be', 'considering', 'pythons', 'inbuilt', 'method', 'we', 'can', 'choose', 'any', 'method', 'based', 'on', 'language', 'library', 'and', 'purpose', 'of', 'modelling', 'there', 'are', '5', 'different', 'types', 'in', 'it', 'characters', 'like', 'periods', 'exclamation', 'point', 'and', 'newline', 'are', 'used', 'to', 'separate', 'the', 'sentences']\n",
      "\n",
      "\n",
      "Sentence token:  ['there are multiple ways we can perform tokenization on given data', ' we will be considering pythons inbuilt method', ' we can choose any method based on language', ' library and purpose of modelling', ' there are 5 different types in it', ' characters like periods', ' exclamation', ' point and newline are used to separate the sentences']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "word=text_to_word_sequence(text)\n",
    "sent=text_to_word_sequence(text, split='.')\n",
    "print('Word token: ', word)\n",
    "print('\\n')\n",
    "print('Sentence token: ', sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de6dc36",
   "metadata": {},
   "source": [
    "# Check token is Alphabet/Punctuation/Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37d4b963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "are \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "multiple \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "ways \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "we \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "can \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "perform \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "tokenization \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "on \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "given \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "data \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ". \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "We \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "will \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "be \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "considering \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "pythons \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "inbuilt \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "method \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ". \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "We \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "can \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "choose \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "any \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "method \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "based \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "on \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "language \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ", \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "library \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "and \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "purpose \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "of \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "modelling \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ". \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "There \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "are \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "5 \n",
      "is_alpha: False \n",
      "is_punct: False \n",
      "like_num: True \n",
      "is_currency: False \n",
      "\n",
      "different \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "types \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "in \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "it \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ". \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "Characters \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "like \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "periods \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ", \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "exclamation \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ", \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "point \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "and \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "newline \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "are \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "used \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "to \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "separate \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "the \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      "sentences \n",
      "is_alpha: True \n",
      "is_punct: False \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n",
      ". \n",
      "is_alpha: False \n",
      "is_punct: True \n",
      "like_num: False \n",
      "is_currency: False \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    print(t,\"\\nis_alpha:\", t.is_alpha,\n",
    "         \"\\nis_punct:\", t.is_punct,\n",
    "         \"\\nlike_num:\", t.like_num,\n",
    "         \"\\nis_currency:\", t.is_currency, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2470d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
